{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vaccine_mediachannels_network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MqGrBR6fph1"
      },
      "source": [
        "## **0. Import Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uuQ-xRrkoV3",
        "outputId": "c5264f8d-9d0f-4ac2-c885-e00319f02aaa"
      },
      "source": [
        "pip install zemberek-python"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting zemberek-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/4f/e006418720e5764a302fd4cc048d8b3003bc80cc2317cdc62254fce3abe6/zemberek_python-0.1.2-py3-none-any.whl (93.6MB)\n",
            "\u001b[K     |████████████████████████████████| 93.6MB 98kB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime>=4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/9c/d5ef93dc1e5a862cae004a64d15425c2a1ae8ba967a08f03dfb11aedf7bf/antlr4-python3-runtime-4.9.2.tar.gz (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from zemberek-python) (1.19.5)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.2-cp37-none-any.whl size=144568 sha256=acf7c42425f54144de38c7a7733adab10f4c81406e41af2af1ffd63a07fd3ed7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/64/ac/8c89516f9cc7341328d7e4a896d2166514798ee24b753f0ca3\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, zemberek-python\n",
            "Successfully installed antlr4-python3-runtime-4.9.2 zemberek-python-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IKNeoivZwxx"
      },
      "source": [
        "import re\n",
        "import string as str\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from zemberek import (\n",
        "    TurkishSpellChecker,\n",
        "    TurkishSentenceNormalizer,\n",
        "    TurkishSentenceExtractor,\n",
        "    TurkishMorphology,\n",
        "    TurkishTokenizer)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quPZWI5Wfo3j"
      },
      "source": [
        "## **1. Load DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um10K4OUasGJ"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/df_4.csv').drop(columns='Unnamed: 0')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdSADMJ1f0ir"
      },
      "source": [
        "## **2. Define Media Channels' Usernames**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHY4SG33cLft"
      },
      "source": [
        "media_channels = [\"TurkishIndy\", \"Irna_Turkish\", \"aawsat_turkce\", \"XHTurkey\", \"AlMonitorTurkce\", \"RudawTurkce\",\n",
        "                 \"CRI_Turkish\", \"sputnik_TR\", \"euronews_tr\", \"AJTurk\", \"dw_turkce\", \"bbcturkce\", \"VOATurkish\"]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRDd4mICi8KD"
      },
      "source": [
        "tr_media_channels = [\"Ahaber\", \"trthaber\", \"anadoluajansi\", \"trthaber\", \"cnnturk\", \"ihacomtr\", \"dhainternet\", \"Sabah\",\n",
        "                     \"Hurriyet\", \"milliyet\", \"gazetesozcu\", \"cumhuriyetgzt\", \"halktvcomtr\", \"FOXhaber\", \"BirGun_Gazetesi\",\n",
        "                     \"Haberturk\", \"ntv\", \"haber7\", \"Stargazete\", \"yenisafak\", \"turkiyegazetesi\", \"takvim\"]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM7Cxd_3f82X"
      },
      "source": [
        "## **3. Preprocess Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z80z7qgygCT3"
      },
      "source": [
        "### **3.1. Remove Emojis From Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nddlSyDMfRak"
      },
      "source": [
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r' ',text)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oejkZo7SlA5j"
      },
      "source": [
        "### **3.2. Tokenize & Clean the Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVPS9pfMj929"
      },
      "source": [
        "zemberek_tokenizer = TurkishTokenizer.DEFAULT\n",
        "\n",
        "def text_preprocess(text):\n",
        "    allowed_types = ['WordWithSymbol', 'Word', 'Punctuation']\n",
        "    \n",
        "    text = deEmojify(text)\n",
        "    text = \" \".join([token.content for token in zemberek_tokenizer.tokenize(text) if token.type_.name in allowed_types])\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = [word for word in text.split()]\n",
        "    return text"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-__zShOgH5f"
      },
      "source": [
        "### **3.2. Tag Each Document**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwV5Fx7kbKWK"
      },
      "source": [
        "tagged_documents = list()\n",
        "for index, row in df.iterrows():\n",
        "  if row.user_username in media_channels:\n",
        "    tagged_documents.append(TaggedDocument(text_preprocess(row.text.lower()), [row.user_username]))"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkgr0OgClKeB"
      },
      "source": [
        "## **4. Create a Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cWaageOgQ49"
      },
      "source": [
        "model = Doc2Vec(tagged_documents, vector_size=20, min_count=3, epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6hKi00XL5Vv"
      },
      "source": [
        "## **5. Network Representation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKyYgm2Po7DP"
      },
      "source": [
        "G = nx.Graph()\n",
        "to_observe = list()\n",
        "\n",
        "for index_1 in range(0, len(model.docvecs.index2entity)):\n",
        "    for index_2 in range(0, len(model.docvecs.index2entity)):\n",
        "        if index_1 != index_2:\n",
        "            weight = 1 / (np.linalg.norm(model.docvecs.vectors_docs[index_1]-model.docvecs.vectors_docs[index_2]))\n",
        "            if weight*10 > 0.5:\n",
        "                G.add_edge(model.docvecs.index2entity[index_1], model.docvecs.index2entity[index_2], weight=weight*10)\n",
        "\n",
        "            to_observe.append((model.docvecs.index2entity[index_1], model.docvecs.index2entity[index_2], weight*10))"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQPmiVILpmva"
      },
      "source": [
        "nx.write_gml(G,'/content/news_channels.gml')"
      ],
      "execution_count": 112,
      "outputs": []
    }
  ]
}